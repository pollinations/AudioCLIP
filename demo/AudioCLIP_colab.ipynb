{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "AudioCLIP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pollinations/AudioCLIP/blob/master/demo/AudioCLIP_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11bbf255"
      },
      "source": [
        "# AudioCLIP Demo\n",
        "\n",
        "Authored by [Andrey Guzhov](https://github.com/AndreyGuzhov)\n",
        "\n",
        "This notebook covers common use cases of AudioCLIP and provides the typical workflow.\n",
        "Below, you will find information on:\n",
        "\n",
        "0. [Binary Assets](#Downloading-Binary-Assets)\n",
        "1. [Required imports](#Imports-&-Constants)\n",
        "2. [Model Instantiation](#Model-Instantiation)\n",
        "3. [Data Transformation](#Audio-&-Image-Transforms)\n",
        "4. Data Loading\n",
        "    * [Audio](#Audio-Loading)\n",
        "    * [Images](#Image-Loading)\n",
        "5. [Preparation of the Input](#Input-Preparation)\n",
        "6. [Acquisition of the Output](#Obtaining-Embeddings)\n",
        "7. [Normalization of Embeddings](#Normalization-of-Embeddings)\n",
        "8. [Calculation of Logit Scales](#Obtaining-Logit-Scales)\n",
        "9. [Computation of Similarities](#Computing-Similarities)\n",
        "10. Performing Tasks\n",
        "    1. [Classification](#Classification)\n",
        "        1. [Audio](#Audio)\n",
        "        2. [Images](#Images)\n",
        "    2. [Querying](#Querying)\n",
        "        1. [Audio by Text](#Audio-by-Text)\n",
        "        2. [Images by Text](#Images-by-Text)\n",
        "        3. [Audio by Images](#Audio-by-Images)\n",
        "        4. [Images by Audio](#Images-by-Audio)"
      ],
      "id": "11bbf255"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6a262a9"
      },
      "source": [
        "## Downloading Binary Assets"
      ],
      "id": "e6a262a9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kYCmCLp6BD8"
      },
      "source": [
        "\n",
        "import subprocess\n",
        "import sys\n",
        "from IPython.display import display\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\n",
        "!pip install siren-pytorch simplejpeg pytorch-ignite visdom"
      ],
      "id": "8kYCmCLp6BD8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEnYTzGp6lua"
      },
      "source": [
        "!git clone https://github.com/pollinations/AudioCLIP\n",
        "sys.path.append('./AudioCLIP/')"
      ],
      "id": "AEnYTzGp6lua",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86673fce"
      },
      "source": [
        "! rm AudioCLIP/assets/bpe_simple_vocab_16e6.txt.gz\n",
        "! rm AudioCLIP/assets/AudioCLIP-Full-Training.pt\n",
        "! wget -P ./AudioCLIP/assets/ https://github.com/AndreyGuzhov/AudioCLIP/releases/download/v0.1/bpe_simple_vocab_16e6.txt.gz\n",
        "! wget -P ./AudioCLIP/assets/ https://github.com/AndreyGuzhov/AudioCLIP/releases/download/v0.1/AudioCLIP-Full-Training.pt"
      ],
      "id": "86673fce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7262c7e"
      },
      "source": [
        "## Imports & Constants"
      ],
      "id": "b7262c7e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70dbdf30"
      },
      "source": [
        "!pip install visdom\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "import simplejpeg\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision as tv\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "#sys.path.append(os.path.abspath(f'{os.getcwd()}/..'))\n",
        "\n",
        "from model import AudioCLIP\n",
        "from utils.transforms import ToTensor1D\n",
        "\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "MODEL_FILENAME = 'AudioCLIP-Full-Training.pt'\n",
        "# derived from ESResNeXt\n",
        "SAMPLE_RATE = 44100\n",
        "# derived from CLIP\n",
        "IMAGE_SIZE = 224\n",
        "IMAGE_MEAN = 0.48145466, 0.4578275, 0.40821073\n",
        "IMAGE_STD = 0.26862954, 0.26130258, 0.27577711\n",
        "\n",
        "LABELS = ['artificial', 'natural', 'dramatic', 'calm']"
      ],
      "id": "70dbdf30",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a327cc6"
      },
      "source": [
        "## Model Instantiation"
      ],
      "id": "6a327cc6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f398f22f"
      },
      "source": [
        "aclp = AudioCLIP(pretrained=f'./AudioCLIP/assets/{MODEL_FILENAME}')"
      ],
      "id": "f398f22f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39421f88"
      },
      "source": [
        "## Audio & Image Transforms"
      ],
      "id": "39421f88"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd4d76b0"
      },
      "source": [
        "audio_transforms = ToTensor1D()\n",
        "\n",
        "image_transforms = tv.transforms.Compose([\n",
        "    tv.transforms.ToTensor(),\n",
        "    tv.transforms.Resize(IMAGE_SIZE, interpolation=Image.BICUBIC),\n",
        "    tv.transforms.CenterCrop(IMAGE_SIZE),\n",
        "    tv.transforms.Normalize(IMAGE_MEAN, IMAGE_STD)\n",
        "])"
      ],
      "id": "dd4d76b0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e5beab9"
      },
      "source": [
        "## Audio Loading\n",
        "Audio samples are drawn from the [ESC-50](https://github.com/karolpiczak/ESC-50) dataset."
      ],
      "id": "1e5beab9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aaa79b8"
      },
      "source": [
        "\n",
        "paths_to_audio = glob.glob('AudioCLIP/demo/audio/*.wav')\n",
        "\n",
        "audio = list()\n",
        "for path_to_audio in paths_to_audio:\n",
        "    track, _ = librosa.load(path_to_audio, sr=SAMPLE_RATE, dtype=np.float32)\n",
        "    #track = track[:len(track)//4]\n",
        "    # compute spectrograms using trained audio-head (fbsp-layer of ESResNeXt)\n",
        "    # thus, the actual time-frequency representation will be visualized\n",
        "    spec = aclp.audio.spectrogram(torch.from_numpy(track.reshape(1, 1, -1)))\n",
        "    spec = np.ascontiguousarray(spec.numpy()).view(np.complex64)\n",
        "    pow_spec = 10 * np.log10(np.abs(spec) ** 2 + 1e-18).squeeze()\n",
        "\n",
        "    audio.append((track, pow_spec))\n",
        "#audio[1][0].shape"
      ],
      "id": "5aaa79b8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b239d3f"
      },
      "source": [
        "fig, axes = plt.subplots(2, len(audio), figsize=(20, 5), dpi=100)\n",
        "\n",
        "for idx in range(len(audio)):\n",
        "    track, pow_spec = audio[idx]\n",
        "\n",
        "    # draw the waveform\n",
        "    librosa.display.waveplot(track, sr=SAMPLE_RATE, ax=axes[0, idx], color='k')\n",
        "    # show the corresponding power spectrogram\n",
        "    axes[1, idx].imshow(pow_spec, origin='lower', aspect='auto', cmap='gray', vmin=-180.0, vmax=20.0)\n",
        "\n",
        "    # modify legend\n",
        "    axes[0, idx].set_title(os.path.basename(paths_to_audio[idx]))\n",
        "    axes[0, idx].set_xlabel('')\n",
        "    axes[0, idx].set_xticklabels([])\n",
        "    axes[0, idx].grid(True)\n",
        "    axes[0, idx].set_ylim(bottom=-1, top=1)\n",
        "\n",
        "    axes[1, idx].set_xlabel('Time (s)')\n",
        "    axes[1, idx].set_xticks(np.linspace(0, pow_spec.shape[1], len(axes[0, idx].get_xticks())))\n",
        "    axes[1, idx].set_xticklabels([f'{tick:.1f}' if tick == int(tick) else '' for tick in axes[0, idx].get_xticks()])\n",
        "    axes[1, idx].set_yticks(np.linspace(0, pow_spec.shape[0] - 1, 5))\n",
        "\n",
        "axes[0, 0].set_ylabel('Amplitude')\n",
        "axes[1, 0].set_ylabel('Filter ID')\n",
        "\n",
        "plt.show()\n",
        "plt.close(fig)\n",
        "\n",
        "for idx, path in enumerate(paths_to_audio):\n",
        "    print(os.path.basename(path))\n",
        "    display(Audio(audio[idx][0], rate=SAMPLE_RATE, embed=True))"
      ],
      "id": "1b239d3f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc725f3f"
      },
      "source": [
        "## Image Loading"
      ],
      "id": "fc725f3f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "014356a0"
      },
      "source": [
        "paths_to_images = glob.glob('AudioCLIP/demo/images/*.jpg')\n",
        "\n",
        "images = list()\n",
        "for path_to_image in paths_to_images:\n",
        "    with open(path_to_image, 'rb') as jpg:\n",
        "        image = simplejpeg.decode_jpeg(jpg.read())\n",
        "        images.append(image)"
      ],
      "id": "014356a0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9a5c522"
      },
      "source": [
        "fig, axes = plt.subplots(2, len(images) // 2, figsize=(16, 4), dpi=100)\n",
        "\n",
        "for idx, jdx in np.ndindex(axes.shape):\n",
        "    # re-arrange order to show the images column-wise\n",
        "    image_idx = np.ravel_multi_index(((jdx,), (idx,)), axes.shape[::-1]).item()\n",
        "    axes[idx, jdx].imshow(images[image_idx])\n",
        "\n",
        "    # modify legend\n",
        "    axes[idx, jdx].axis('off')\n",
        "    axes[idx, jdx].set_title(os.path.basename(paths_to_images[image_idx]))\n",
        "\n",
        "plt.show()\n",
        "plt.close(fig)"
      ],
      "id": "f9a5c522",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7c11976"
      },
      "source": [
        "## Input Preparation"
      ],
      "id": "e7c11976"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbf1059b"
      },
      "source": [
        "# AudioCLIP handles raw audio on input, so the input shape is [batch x channels x duration]\n",
        "audio = torch.stack([audio_transforms(track.reshape(1, -1)) for track, _ in audio])\n",
        "\n",
        "# standard channel-first shape [batch x channels x height x width]\n",
        "images = torch.stack([image_transforms(image) for image in images])\n",
        "# textual input is processed internally, so no need to transform it beforehand\n",
        "text = [[label] for label in LABELS]\n"
      ],
      "id": "bbf1059b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYKo2ZbmRpdM"
      },
      "source": [
        "\n",
        "print(text)\n",
        "print(audio.shape)\n",
        "print(images.shape)"
      ],
      "id": "RYKo2ZbmRpdM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afbb9ef5"
      },
      "source": [
        "## Obtaining Embeddings\n",
        "For the sake of clarity, all three modalities are processed separately."
      ],
      "id": "afbb9ef5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60c71e0d"
      },
      "source": [
        "# AudioCLIP's output: Tuple[Tuple[Features, Logits], Loss]\n",
        "# Features = Tuple[AudioFeatures, ImageFeatures, TextFeatures]\n",
        "# Logits = Tuple[AudioImageLogits, AudioTextLogits, ImageTextLogits]\n",
        "print(audio.shape)\n",
        "((audio_features, _, _), _), _ = aclp(audio=audio)\n",
        "((_, image_features, _), _), _ = aclp(image=images)\n",
        "((_, _, text_features), _), _ = aclp(text=text)"
      ],
      "id": "60c71e0d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4e45ed0"
      },
      "source": [
        "## Normalization of Embeddings\n",
        "The AudioCLIP's output is normalized using L<sub>2</sub>-norm"
      ],
      "id": "e4e45ed0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9758c7c"
      },
      "source": [
        "audio_features = audio_features / torch.linalg.norm(audio_features, dim=-1, keepdim=True)\n",
        "image_features = image_features / torch.linalg.norm(image_features, dim=-1, keepdim=True)\n",
        "text_features = text_features / torch.linalg.norm(text_features, dim=-1, keepdim=True)"
      ],
      "id": "f9758c7c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c92adfb5"
      },
      "source": [
        "## Obtaining Logit Scales\n",
        "Outputs of the text-, image- and audio-heads are made consistent using dedicated scaling terms for each pair of modalities.\n",
        "The scaling factors are clamped between 1.0 and 100.0."
      ],
      "id": "c92adfb5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12a89e64"
      },
      "source": [
        "scale_audio_image = torch.clamp(aclp.logit_scale_ai.exp(), min=1.0, max=100.0)\n",
        "scale_audio_text = torch.clamp(aclp.logit_scale_at.exp(), min=1.0, max=100.0)\n",
        "scale_image_text = torch.clamp(aclp.logit_scale.exp(), min=1.0, max=100.0)"
      ],
      "id": "12a89e64",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32e3dfd0"
      },
      "source": [
        "## Computing Similarities\n",
        "Similarities between different representations of a same concept are computed using [scaled](#Obtaining-Logit-Scales) dot product (cosine similarity)."
      ],
      "id": "32e3dfd0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3121148"
      },
      "source": [
        "logits_audio_image = scale_audio_image * audio_features @ image_features.T\n",
        "logits_audio_text = scale_audio_text * audio_features @ text_features.T\n",
        "logits_image_text = scale_image_text * image_features @ text_features.T"
      ],
      "id": "c3121148",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7a0dfa2"
      },
      "source": [
        "## Classification\n",
        "This task is a specific case of a more general one, which is [querying](#Querying).\n",
        "However, this setup is mentioned as a standalone because it demonstrates clearly how to perform usual classification (including [zero-shot inference](https://github.com/openai/CLIP#zero-shot-prediction)) using AudioCLIP."
      ],
      "id": "b7a0dfa2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bdd15af"
      },
      "source": [
        "### Audio"
      ],
      "id": "5bdd15af"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ccc74da"
      },
      "source": [
        "print('\\t\\tFilename, Audio\\t\\t\\tTextual Label (Confidence)', end='\\n\\n')\n",
        "\n",
        "# calculate model confidence\n",
        "confidence = logits_audio_text.softmax(dim=1)\n",
        "for audio_idx in range(len(paths_to_audio)):\n",
        "    # acquire Top-3 most similar results\n",
        "    conf_values, ids = confidence[audio_idx].topk(3)\n",
        "\n",
        "    # format output strings\n",
        "    query = f'{os.path.basename(paths_to_audio[audio_idx]):>30s} ->\\t\\t'\n",
        "    results = ', '.join([f'{LABELS[i]:>15s} ({v:06.2%})' for v, i in zip(conf_values, ids)])\n",
        "\n",
        "    print(query + results)"
      ],
      "id": "6ccc74da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96fc0350"
      },
      "source": [
        "### Images"
      ],
      "id": "96fc0350"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "d020de4a"
      },
      "source": [
        "print('\\tFilename, Image\\t\\t\\tTextual Label (Confidence)', end='\\n\\n')\n",
        "\n",
        "# calculate model confidence\n",
        "confidence = logits_image_text.softmax(dim=1)\n",
        "for image_idx in range(len(paths_to_images)):\n",
        "    # acquire Top-3 most similar results\n",
        "    conf_values, ids = confidence[image_idx].topk(3)\n",
        "\n",
        "    # format output strings\n",
        "    query = f'{os.path.basename(paths_to_images[image_idx]):>20s} ->\\t\\t'\n",
        "    results = ', '.join([f'{LABELS[i]:>20s} ({v:06.2%})' for v, i in zip(conf_values, ids)])\n",
        "\n",
        "    print(query + results)"
      ],
      "id": "d020de4a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f49a1a06"
      },
      "source": [
        "## Querying"
      ],
      "id": "f49a1a06"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b939334c"
      },
      "source": [
        "### Audio by Text"
      ],
      "id": "b939334c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "d1e0b4ff"
      },
      "source": [
        "print('\\t\\tTextual Label\\t\\tFilename, Audio (Confidence)', end='\\n\\n')\n",
        "\n",
        "# calculate model confidence\n",
        "confidence = logits_audio_text.softmax(dim=0)\n",
        "for label_idx in range(len(LABELS)):\n",
        "    # acquire Top-2 most similar results\n",
        "    conf_values, ids = confidence[:, label_idx].topk(2)\n",
        "\n",
        "    # format output strings\n",
        "    query = f'{LABELS[label_idx]:>25s} ->\\t\\t'\n",
        "    results = ', '.join([f'{os.path.basename(paths_to_audio[i]):>30s} ({v:06.2%})' for v, i in zip(conf_values, ids)])\n",
        "\n",
        "    print(query + results)"
      ],
      "id": "d1e0b4ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19165854"
      },
      "source": [
        "### Images by Text"
      ],
      "id": "19165854"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7133e95"
      },
      "source": [
        "print('\\tTextual Label\\t\\t\\tFilename, Image (Confidence)', end='\\n\\n')\n",
        "\n",
        "# calculate model confidence\n",
        "confidence = logits_image_text.softmax(dim=0)\n",
        "for label_idx in range(len(LABELS)):\n",
        "    # acquire Top-3 most similar results\n",
        "    conf_values, ids = confidence[:, label_idx].topk(3)\n",
        "\n",
        "    # format output strings\n",
        "    query = f'{LABELS[label_idx]:>20s} ->\\t\\t'\n",
        "    results = ', '.join([f'{os.path.basename(paths_to_images[i]):>20s} ({v:>06.2%})' for v, i in zip(conf_values, ids)])\n",
        "\n",
        "    print(query + results)"
      ],
      "id": "b7133e95",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fae8c15"
      },
      "source": [
        "### Audio by Images"
      ],
      "id": "0fae8c15"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "cea504a0"
      },
      "source": [
        "print('\\tTextual Label\\t\\t\\tFilename, Image (Confidence)', end='\\n\\n')\n",
        "\n",
        "# calculate model confidence\n",
        "confidence = logits_audio_image.softmax(dim=0)\n",
        "for image_idx in range(len(paths_to_images)):\n",
        "    # acquire Top-2 most similar results\n",
        "    conf_values, ids = confidence[:, image_idx].topk(2)\n",
        "\n",
        "    # format output strings\n",
        "    query = f'{os.path.basename(paths_to_images[image_idx]):>25s} ->\\t\\t'\n",
        "    results = ', '.join([f'{os.path.basename(paths_to_audio[i]):>30s} ({v:06.2%})' for v, i in zip(conf_values, ids)])\n",
        "\n",
        "    print(query + results)"
      ],
      "id": "cea504a0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb44e32e"
      },
      "source": [
        "### Images by Audio"
      ],
      "id": "bb44e32e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "eabe3821"
      },
      "source": [
        "print('\\tTextual Label\\t\\t\\tFilename, Image (Confidence)', end='\\n\\n')\n",
        "\n",
        "# calculate model confidence\n",
        "confidence = logits_audio_image.softmax(dim=1)\n",
        "for audio_idx in range(len(paths_to_audio)):\n",
        "    # acquire Top-3 most similar results\n",
        "    conf_values, ids = confidence[audio_idx].topk(3)\n",
        "\n",
        "    # format output strings\n",
        "    query = f'{os.path.basename(paths_to_audio[audio_idx]):>30s} ->\\t\\t'\n",
        "    results = ', '.join([f'{os.path.basename(paths_to_images[i]):>15s} ({v:06.2%})' for v, i in zip(conf_values, ids)])\n",
        "\n",
        "    print(query + results)"
      ],
      "id": "eabe3821",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4g39mCo_9up"
      },
      "source": [
        ""
      ],
      "id": "q4g39mCo_9up",
      "execution_count": null,
      "outputs": []
    }
  ]
}